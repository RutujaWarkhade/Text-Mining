{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63ab62f5-d404-4138-96be-02f312e98572",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U spacy\n",
    "#conda install -c conda-forge spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4210fdbc-8e2e-4e05-8b55-b042791ca990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a378855-e68d-4428-9465-af3fdb531eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for regex we use r' '\n",
    "#similarly in spacy we use u' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee901dfa-a5d9-467a-85a0-2b722a2e3c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "U.S.\n",
      "startup\n",
      "for\n",
      "$\n",
      "6\n",
      "million\n"
     ]
    }
   ],
   "source": [
    "#Import spacy and load the language library\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#create a Doc object\n",
    "doc = nlp(u'Tesla is looking at buying U.S. startup for $6 million')\n",
    "\n",
    "#print each token sepaerately\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "538ae5b7-77b9-4aec-b7d9-f728f1f35326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla PROPN\n",
      "is AUX\n",
      "looking VERB\n",
      "at ADP\n",
      "buying VERB\n",
      "U.S. PROPN\n",
      "startup NOUN\n",
      "for ADP\n",
      "$ SYM\n",
      "6 NUM\n",
      "million NUM\n"
     ]
    }
   ],
   "source": [
    "#Import spacy and load the language library\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#create a Doc object\n",
    "doc = nlp(u'Tesla is looking at buying U.S. startup for $6 million')\n",
    "\n",
    "#print each token sepaerately\n",
    "for token in doc:\n",
    "    print(token.text,token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3263a9a-878a-457f-a7f0-4d58e54048c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla PROPN nsubj\n",
      "is AUX aux\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.S. PROPN compound\n",
      "startup NOUN dobj\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "6 NUM compound\n",
      "million NUM pobj\n"
     ]
    }
   ],
   "source": [
    "#Import spacy and load the language library\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#create a Doc object\n",
    "doc = nlp(u'Tesla is looking at buying U.S. startup for $6 million')\n",
    "\n",
    "#print each token sepaerately\n",
    "for token in doc:\n",
    "    print(token.text,token.pos_, token.dep_)\n",
    "#predict syntactic dependancies\n",
    "#predicting syntactic dependancies involves\n",
    "#identifying the grammatical structure of a \n",
    "#sentence by determining how different  words relate to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa61e73e-23d9-4f96-81c9-11b73a3a5844",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "find information on google and siginificance about parts of speech, token postion,\n",
    "token dependancy \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5b8566b-e89d-4449-ab1d-dc467020ceec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x239aaae3d10>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x239aaae13d0>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x239aab15cb0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x239abbb3710>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x239aaaa9f90>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x239aab15d90>)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b845707-7c3e-40ce-bfba-c35ba15ad526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8143fb6-dbd6-49f4-99c9-0e82df482fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla PROPN nsubj\n",
      "is AUX aux\n",
      "n't PART neg\n",
      "  SPACE dep\n",
      "looking VERB ROOT\n",
      "into ADP prep\n",
      "startups NOUN pobj\n",
      "anymore ADV advmod\n"
     ]
    }
   ],
   "source": [
    "#tokenization is the first step in processing text is to split up all the components parts\n",
    "#(words and punctuation) into \"tokens\"\n",
    "doc2 = nlp(u\"Tesla isn't  looking into startups anymore\")\n",
    "for token in doc2:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d132ac38-6b2c-4761-9268-a995343cbf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice how isn't has been split into two tokens. spaCy recoginzes both the root verb is and the negation\n",
    "# attached to it. Notice also that both the extended whitespace and the period at the end of the sentence are assigned their own tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebf4a1cb-4e73-4e74-b97e-f618de71390d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tesla isn't  looking into startups anymore"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6acf8462-866a-43a9-8c0e-0bb762e25e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tesla"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b7345f-a2cc-4335-a3a5-1160593fa995",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://spacy.io/usage/liguistic-features#pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c55995-ddd4-452f-9617-0a870d1673d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parts of speech the next step after spliting the text up into tokens in to assign parts of speech.\n",
    "\n",
    "for full list of POS tag list https://spacy.io/api/annotation#pos-tagging\n",
    "\n",
    "for dependancy https://spacy.io/api/annotation#dependancy-parsing\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0bdf75-e65c-4220-be99-0232155ff928",
   "metadata": {},
   "outputs": [],
   "source": [
    "#28 AUGUST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db108695-0746-45aa-89c0-eb5c3022430c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PROPN'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[0].pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0aa7b7a-c0fa-4540-95fb-e49a55d35280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nsubj'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[0].dep_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6be6fb85-bf32-4d89-80de-3df1dd4fcbdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nominal subject'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('nsubj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe43d878-be6a-46ef-ab85-a423464d1879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'proper noun'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('PROPN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81450e2c-40b0-4b64-8f39-735a82a694b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking\n",
      "look\n"
     ]
    }
   ],
   "source": [
    "#Lemma (the base from of the word):\n",
    "print(doc2[4].text)\n",
    "print(doc2[4].lemma_)\n",
    "#lemma means lemmitizer which gives original form of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7485c0-0a32-4758-91da-99edfe841be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Spans\n",
    "large Doc objects can be hard to work with at times. A span is a slice of Doc object \n",
    "in the form Doc[start:stop]\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee8d2382-9118-4c43-95a2-7a3ead0a52d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = nlp(u'Although commonly attributed to John Lennon from his song \"Beautiful Boy\",\\\n",
    "the phrase \"Life is what happens to us while we are making other plans\" was written by\\\n",
    "cartoonist Allen Saunders and publised in Reader\\'s Digest in 1957, when Lennon was 17.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e56cc970-239f-4d4e-91ee-b6f18634ec3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what happens to us while we are making other plans\" was written bycartoonist\n"
     ]
    }
   ],
   "source": [
    "life_quote = doc3[16:30]\n",
    "print(life_quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4b90254-63b8-49bc-bb81-4c6e452731be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(life_quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0bfb0b-9909-4e75-a1ba-15f47a1163ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "certain tokens inside a Doc \n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32b8e2c9-ec0f-46fc-9f8a-16730432f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence tokenization\n",
    "doc4 = nlp(u'This is the first sentence. This is author sentence. This is the last sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ce11fd4-dc8c-4a88-9ae5-4fb970ff29a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first sentence.\n",
      "This is author sentence.\n",
      "This is the last sentence\n"
     ]
    }
   ],
   "source": [
    "for sent in doc4.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c1495b9-78c0-4db1-a6a7-ebb641dbb366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"We're moving to L.A.!\"\n"
     ]
    }
   ],
   "source": [
    "mystring='\"We\\'re moving to L.A.!\"'\n",
    "print(mystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54ca09cb-084e-477d-bc41-2c29c850e38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" |We |'re |moving |to |L.A. |! |\" |"
     ]
    }
   ],
   "source": [
    "#create a Doc object and explore tokens\n",
    "doc = nlp(mystring)\n",
    "for token in doc:\n",
    "    print(token.text, end=' |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c316559-6546-406d-8bd1-694749c463db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We | 're | the | here | to | help | ! | Send | snail | - | mail | , | email | support@oursite.com | or | visit | us | at | http://www.oursite.com | ! | "
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u\"We're the here to help! Send snail-mail, email support@oursite.com or visit us at http://www.oursite.com!\")\n",
    "for token in doc2:\n",
    "    print(token.text, end=' | ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f003dde-3aaa-4d43-a65e-f15f61598556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "5\n",
      "km\n",
      "NYC\n",
      "cab\n",
      "ride\n",
      "costs\n",
      "$\n",
      "10.30\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(u'A 5km NYC cab ride costs $10.30')\n",
    "for t in doc3:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f19a95ac-f9e3-4322-83b7-fc521f76205e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let\n",
      "'s\n",
      "visit\n",
      "St.\n",
      "Louis\n",
      "in\n",
      "the\n",
      "U.S.\n",
      "next\n",
      "year\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp(u\"Let's visit St. Louis in the U.S. next year.\")\n",
    "for t in doc4:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee49bc83-4bd1-43e7-b4c7-5ae9af1fda65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
